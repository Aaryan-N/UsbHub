{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObYSxvYRJ2fuIjCQpLKJfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aaryan-N/UsbHub/blob/main/TrumpGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my first time using python (The syntax is pretty easy as I know javascript and java) and my first time building an LLM! Thanks to this legend: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=580s I somehow managed to turn a 2 hour tutorial into 5 hours RIP"
      ],
      "metadata": {
        "id": "-MpsKBAiGjiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grab the Trump tweet data set from my repo. Also run this on a T4 setup for fast speed and just hit the run all under the runtime tab."
      ],
      "metadata": {
        "id": "foKCOYPtvwPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5VtY9zUvlRW",
        "outputId": "844f00de-69ab-4db1-84b1-3cef92ffcc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-08 00:58:08--  https://github.com/Aaryan-N/sentimentllm/raw/main/TrumpTwitterAllProper.txt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Aaryan-N/sentimentllm/main/TrumpTwitterAllProper.txt [following]\n",
            "--2024-09-08 00:58:08--  https://raw.githubusercontent.com/Aaryan-N/sentimentllm/main/TrumpTwitterAllProper.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3367981 (3.2M) [text/plain]\n",
            "Saving to: ‘TrumpTwitterAllProper.txt.3’\n",
            "\n",
            "TrumpTwitterAllProp 100%[===================>]   3.21M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-09-08 00:58:08 (108 MB/s) - ‘TrumpTwitterAllProper.txt.3’ saved [3367981/3367981]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Aaryan-N/sentimentllm/raw/main/TrumpTwitterAllProper.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set variables"
      ],
      "metadata": {
        "id": "I9zr_Cey5iDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "batch_size = 16 # Amount of tokens in a batch\n",
        "block_size = 34 # Amount of batchs in a block as per my understanding\n",
        "max_iters = 30000 # Increase me to get a better result\n",
        "eval_interval = 100 # How often you want the program to print out the train loss and val loss\n",
        "l_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "kKFZ50u35gqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open up the file and print the length of the text\n"
      ],
      "metadata": {
        "id": "2XQtkbANxaxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('TrumpTwitterAllProper.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0blSgdvxcQK",
        "outputId": "cb4353c1-8e34-4cbb-a17f-09bfcdf48b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3325571"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Colab has managed to read it"
      ],
      "metadata": {
        "id": "fnx1dpetzTT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-oT-wa7zX2D",
        "outputId": "ffa6c040-58bc-4293-ec1e-79bff0f263d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet\n",
            "\"If the press would cover me accurately & honorably, I would have far less reason to \"\"tweet.\"\" Sadly, I don't know if that will ever happen!\"\n",
            "I am thrilled to nominate Dr. @RealBenCarson as our next Secretary of the US Dept. of Housing and Urban Development… https://t.co/OJKuDFhP3r\n",
            "their country (the U.S. doesn't tax them) or to build a massive military complex in the middle of the South China Sea? I don't think so!\n",
            "\"Did China ask us if it was OK to devalue their currency (making it hard for our companies to compete), heavily tax our products going into..\"\n",
            "\".@FoxNews will be re-running \"\"Objectified: Donald Trump,\"\" the ratings hit produced by the great Harvey Levin of TMZ, at 8:00 P.M. Enjoy!\"\n",
            "The Green Party just dropped its recount suit in Pennsylvania and is losing votes in Wisconsin recount. Just a Stein scam to raise money!\n",
            "expensive mistake! THE UNITED STATES IS OPEN FOR BUSINESS\n",
            "\"these companies are able to move between all 50 states, with no tax or tariff being charged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define encoding and decoding functions and characters"
      ],
      "metadata": {
        "id": "zCoWBdHSztI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text))) # Create a list of the text and then sort it\n",
        "vocab_size = len(chars) # Length of the sorted list\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # Step to iterate over the characters\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # Encoder Function\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder Function"
      ],
      "metadata": {
        "id": "ubwk9HmRzue3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add torch data tensors so Torch can then use it"
      ],
      "metadata": {
        "id": "oHiKdSTfzfoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long) # Pass encoded (tokenized) data to the tensor function"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qTwU3QfGzj8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data for training and blocking"
      ],
      "metadata": {
        "id": "bzNMYxha0DDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val = int(0.9*len(data)) # 90% of this will be used for training and 10% used to ensure it isn't just copying the data set\n",
        "train_data = data[:train_val]\n",
        "val_data = data[train_val:]\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z0IhSXv0PEs",
        "outputId": "b5a521fb-523a-43b6-b532-9e0e55b2fae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([53, 86, 68, 68, 83,  0,  3, 42, 69,  1, 83, 71, 68,  1, 79, 81, 68, 82,\n",
              "        82,  1, 86, 78, 84, 75, 67,  1, 66, 78, 85, 68, 81,  1, 76, 68,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blocking now (Makes a lot of sense after he explained it. I'm new to python and machine learning in general so very exciting stuff!)"
      ],
      "metadata": {
        "id": "9eT5f6A-2xwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1432)\n",
        "\n",
        "# So this basically splits the data into blocks so we don't just load the entire dataset onto the transformer because it would be very hardware intensive\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data #  If training then move into training sequence\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Iterate through all the data in blocks as defined above\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "print('Success!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSMShmv-29Yg",
        "outputId": "c4015899-4b5e-4dae-fa2e-b737d586be04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "01jFYxAT6Ow2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # Call the training function\n",
        "    return out # The output"
      ],
      "metadata": {
        "id": "TVWmFKPw6NHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heads of self-attention (Nodes talking to each other)"
      ],
      "metadata": {
        "id": "8xf6NXa16cLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    # Single head / Pretty chill and understandable\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) # These functions multiply the matrix's generated\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Triangulate the matrix so there is a diagonal of only zeros\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (Block Size, Time, Context)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) / As we cannnot multiply these we must transpose the matrix\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Convert all zeros in the diagonal into negative infinity\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # Aggregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "# After this comment, it is really difficult for me to understand this he didn't really cover this in his video. I haven't even learnt all this in school :(\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "wl7OZTX_6b-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding bigram (This is probably the only part I have close to no clue how it works, so much math. How are we putting gradients on these numbers?) Also bigram isn't a model, it's just a name of this kind of model"
      ],
      "metadata": {
        "id": "pTfVlDds3dL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1) # Apply softmax\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Number of parameters in the TrumpGPT model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters.', 'We not making ChatGPT with this one')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufx2JKIM3cVz",
        "outputId": "ab91f912-e40f-460a-a005-34f30fdd4292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.216436 M parameters. We not making ChatGPT with this one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model and printing the loss value (More iterations means lower loss and train values aka better result. Redefine the iterations in the declare variables code block at the top) 20000 iterations took me about 17 minutes so keep this in mind!"
      ],
      "metadata": {
        "id": "RMPY4zqz5RvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=l_rate) # Use the AdamW optimizer\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # Evaluate the loss on training and value sets occasionally and print\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Training loss {losses['train']:.5f}, Value loss {losses['val']:.5f}\")\n",
        "    # We want these values to be as low as possible meaning the model is losing less data and making less mistakes\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRK3EdSO5Vk4",
        "outputId": "edfd3ce3-d358-4d21-8ba9-8b921f7f812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Training loss 4.99368, Value loss 4.99021\n",
            "Step 100: Training loss 3.87605, Value loss 3.84083\n",
            "Step 200: Training loss 3.58624, Value loss 3.54354\n",
            "Step 300: Training loss 3.44978, Value loss 3.43299\n",
            "Step 400: Training loss 3.31072, Value loss 3.32000\n",
            "Step 500: Training loss 3.18375, Value loss 3.22486\n",
            "Step 600: Training loss 3.08875, Value loss 3.15033\n",
            "Step 700: Training loss 3.01187, Value loss 3.08618\n",
            "Step 800: Training loss 2.95984, Value loss 3.04529\n",
            "Step 900: Training loss 2.91341, Value loss 2.99445\n",
            "Step 1000: Training loss 2.88269, Value loss 2.97105\n",
            "Step 1100: Training loss 2.84081, Value loss 2.94508\n",
            "Step 1200: Training loss 2.81898, Value loss 2.92351\n",
            "Step 1300: Training loss 2.79155, Value loss 2.89683\n",
            "Step 1400: Training loss 2.76422, Value loss 2.88668\n",
            "Step 1500: Training loss 2.74400, Value loss 2.86644\n",
            "Step 1600: Training loss 2.72783, Value loss 2.84873\n",
            "Step 1700: Training loss 2.71071, Value loss 2.82417\n",
            "Step 1800: Training loss 2.69920, Value loss 2.82372\n",
            "Step 1900: Training loss 2.68532, Value loss 2.80293\n",
            "Step 2000: Training loss 2.66983, Value loss 2.78529\n",
            "Step 2100: Training loss 2.65288, Value loss 2.78585\n",
            "Step 2200: Training loss 2.62530, Value loss 2.78029\n",
            "Step 2300: Training loss 2.62447, Value loss 2.77254\n",
            "Step 2400: Training loss 2.62260, Value loss 2.75184\n",
            "Step 2500: Training loss 2.60118, Value loss 2.73910\n",
            "Step 2600: Training loss 2.58650, Value loss 2.72632\n",
            "Step 2700: Training loss 2.58846, Value loss 2.72666\n",
            "Step 2800: Training loss 2.56097, Value loss 2.71904\n",
            "Step 2900: Training loss 2.54762, Value loss 2.70346\n",
            "Step 3000: Training loss 2.54480, Value loss 2.68173\n",
            "Step 3100: Training loss 2.52891, Value loss 2.68201\n",
            "Step 3200: Training loss 2.51867, Value loss 2.66020\n",
            "Step 3300: Training loss 2.51231, Value loss 2.66150\n",
            "Step 3400: Training loss 2.50531, Value loss 2.65507\n",
            "Step 3500: Training loss 2.49670, Value loss 2.64495\n",
            "Step 3600: Training loss 2.48156, Value loss 2.62782\n",
            "Step 3700: Training loss 2.47654, Value loss 2.62716\n",
            "Step 3800: Training loss 2.47733, Value loss 2.61378\n",
            "Step 3900: Training loss 2.45898, Value loss 2.60248\n",
            "Step 4000: Training loss 2.45400, Value loss 2.59326\n",
            "Step 4100: Training loss 2.43572, Value loss 2.59328\n",
            "Step 4200: Training loss 2.40987, Value loss 2.59044\n",
            "Step 4300: Training loss 2.44060, Value loss 2.58458\n",
            "Step 4400: Training loss 2.41285, Value loss 2.56853\n",
            "Step 4500: Training loss 2.41431, Value loss 2.55205\n",
            "Step 4600: Training loss 2.38411, Value loss 2.55744\n",
            "Step 4700: Training loss 2.39835, Value loss 2.54187\n",
            "Step 4800: Training loss 2.38878, Value loss 2.54724\n",
            "Step 4900: Training loss 2.38357, Value loss 2.53980\n",
            "Step 5000: Training loss 2.38712, Value loss 2.53756\n",
            "Step 5100: Training loss 2.35223, Value loss 2.52183\n",
            "Step 5200: Training loss 2.35213, Value loss 2.52124\n",
            "Step 5300: Training loss 2.36662, Value loss 2.50317\n",
            "Step 5400: Training loss 2.33338, Value loss 2.50338\n",
            "Step 5500: Training loss 2.33448, Value loss 2.50559\n",
            "Step 5600: Training loss 2.32620, Value loss 2.49509\n",
            "Step 5700: Training loss 2.32152, Value loss 2.49039\n",
            "Step 5800: Training loss 2.32540, Value loss 2.47517\n",
            "Step 5900: Training loss 2.32109, Value loss 2.47402\n",
            "Step 6000: Training loss 2.29623, Value loss 2.47811\n",
            "Step 6100: Training loss 2.32199, Value loss 2.47377\n",
            "Step 6200: Training loss 2.29317, Value loss 2.45806\n",
            "Step 6300: Training loss 2.29559, Value loss 2.45607\n",
            "Step 6400: Training loss 2.29955, Value loss 2.45097\n",
            "Step 6500: Training loss 2.28422, Value loss 2.45306\n",
            "Step 6600: Training loss 2.25791, Value loss 2.42922\n",
            "Step 6700: Training loss 2.26758, Value loss 2.44308\n",
            "Step 6800: Training loss 2.27079, Value loss 2.42592\n",
            "Step 6900: Training loss 2.27684, Value loss 2.42139\n",
            "Step 7000: Training loss 2.27073, Value loss 2.41949\n",
            "Step 7100: Training loss 2.25240, Value loss 2.41806\n",
            "Step 7200: Training loss 2.26932, Value loss 2.41133\n",
            "Step 7300: Training loss 2.24780, Value loss 2.41431\n",
            "Step 7400: Training loss 2.23816, Value loss 2.39970\n",
            "Step 7500: Training loss 2.24386, Value loss 2.38640\n",
            "Step 7600: Training loss 2.23055, Value loss 2.38917\n",
            "Step 7700: Training loss 2.22666, Value loss 2.39553\n",
            "Step 7800: Training loss 2.22005, Value loss 2.39742\n",
            "Step 7900: Training loss 2.22596, Value loss 2.37443\n",
            "Step 8000: Training loss 2.22721, Value loss 2.37269\n",
            "Step 8100: Training loss 2.22161, Value loss 2.37740\n",
            "Step 8200: Training loss 2.21681, Value loss 2.37704\n",
            "Step 8300: Training loss 2.20072, Value loss 2.37420\n",
            "Step 8400: Training loss 2.18931, Value loss 2.36245\n",
            "Step 8500: Training loss 2.20321, Value loss 2.35500\n",
            "Step 8600: Training loss 2.19892, Value loss 2.36278\n",
            "Step 8700: Training loss 2.19567, Value loss 2.35124\n",
            "Step 8800: Training loss 2.19051, Value loss 2.34738\n",
            "Step 8900: Training loss 2.18133, Value loss 2.33591\n",
            "Step 9000: Training loss 2.18680, Value loss 2.34843\n",
            "Step 9100: Training loss 2.18394, Value loss 2.34639\n",
            "Step 9200: Training loss 2.15785, Value loss 2.33556\n",
            "Step 9300: Training loss 2.16696, Value loss 2.34066\n",
            "Step 9400: Training loss 2.16559, Value loss 2.32541\n",
            "Step 9500: Training loss 2.15169, Value loss 2.33064\n",
            "Step 9600: Training loss 2.17135, Value loss 2.32528\n",
            "Step 9700: Training loss 2.16335, Value loss 2.32420\n",
            "Step 9800: Training loss 2.15987, Value loss 2.31696\n",
            "Step 9900: Training loss 2.13990, Value loss 2.32146\n",
            "Step 10000: Training loss 2.14743, Value loss 2.31387\n",
            "Step 10100: Training loss 2.13876, Value loss 2.31989\n",
            "Step 10200: Training loss 2.13204, Value loss 2.31731\n",
            "Step 10300: Training loss 2.12534, Value loss 2.30400\n",
            "Step 10400: Training loss 2.14021, Value loss 2.29193\n",
            "Step 10500: Training loss 2.13230, Value loss 2.30890\n",
            "Step 10600: Training loss 2.13472, Value loss 2.28460\n",
            "Step 10700: Training loss 2.13008, Value loss 2.30013\n",
            "Step 10800: Training loss 2.12443, Value loss 2.29663\n",
            "Step 10900: Training loss 2.12533, Value loss 2.29397\n",
            "Step 11000: Training loss 2.12370, Value loss 2.28559\n",
            "Step 11100: Training loss 2.12548, Value loss 2.28100\n",
            "Step 11200: Training loss 2.11517, Value loss 2.30025\n",
            "Step 11300: Training loss 2.10794, Value loss 2.29121\n",
            "Step 11400: Training loss 2.12380, Value loss 2.27708\n",
            "Step 11500: Training loss 2.11387, Value loss 2.26152\n",
            "Step 11600: Training loss 2.10925, Value loss 2.26662\n",
            "Step 11700: Training loss 2.09589, Value loss 2.28183\n",
            "Step 11800: Training loss 2.08788, Value loss 2.27666\n",
            "Step 11900: Training loss 2.09534, Value loss 2.26408\n",
            "Step 12000: Training loss 2.10859, Value loss 2.26694\n",
            "Step 12100: Training loss 2.08613, Value loss 2.25098\n",
            "Step 12200: Training loss 2.08092, Value loss 2.27043\n",
            "Step 12300: Training loss 2.09137, Value loss 2.25637\n",
            "Step 12400: Training loss 2.10126, Value loss 2.25713\n",
            "Step 12500: Training loss 2.08277, Value loss 2.23679\n",
            "Step 12600: Training loss 2.07761, Value loss 2.25335\n",
            "Step 12700: Training loss 2.07001, Value loss 2.23086\n",
            "Step 12800: Training loss 2.07494, Value loss 2.24114\n",
            "Step 12900: Training loss 2.06572, Value loss 2.24004\n",
            "Step 13000: Training loss 2.07120, Value loss 2.23511\n",
            "Step 13100: Training loss 2.07611, Value loss 2.24069\n",
            "Step 13200: Training loss 2.06310, Value loss 2.23783\n",
            "Step 13300: Training loss 2.06747, Value loss 2.23167\n",
            "Step 13400: Training loss 2.06459, Value loss 2.23646\n",
            "Step 13500: Training loss 2.06445, Value loss 2.22690\n",
            "Step 13600: Training loss 2.05646, Value loss 2.21603\n",
            "Step 13700: Training loss 2.04234, Value loss 2.21739\n",
            "Step 13800: Training loss 2.05504, Value loss 2.23496\n",
            "Step 13900: Training loss 2.03943, Value loss 2.21586\n",
            "Step 14000: Training loss 2.07179, Value loss 2.22025\n",
            "Step 14100: Training loss 2.04408, Value loss 2.21330\n",
            "Step 14200: Training loss 2.03993, Value loss 2.20878\n",
            "Step 14300: Training loss 2.04910, Value loss 2.21566\n",
            "Step 14400: Training loss 2.04980, Value loss 2.20163\n",
            "Step 14500: Training loss 2.03958, Value loss 2.21540\n",
            "Step 14600: Training loss 2.05076, Value loss 2.20069\n",
            "Step 14700: Training loss 2.04424, Value loss 2.19946\n",
            "Step 14800: Training loss 2.02245, Value loss 2.19416\n",
            "Step 14900: Training loss 2.03751, Value loss 2.19159\n",
            "Step 15000: Training loss 2.03034, Value loss 2.19588\n",
            "Step 15100: Training loss 2.02489, Value loss 2.18355\n",
            "Step 15200: Training loss 2.01289, Value loss 2.18595\n",
            "Step 15300: Training loss 2.01732, Value loss 2.19600\n",
            "Step 15400: Training loss 2.00535, Value loss 2.18074\n",
            "Step 15500: Training loss 2.02143, Value loss 2.19130\n",
            "Step 15600: Training loss 2.02720, Value loss 2.18909\n",
            "Step 15700: Training loss 2.01901, Value loss 2.17317\n",
            "Step 15800: Training loss 1.99683, Value loss 2.18299\n",
            "Step 15900: Training loss 2.00888, Value loss 2.17607\n",
            "Step 16000: Training loss 1.99773, Value loss 2.17058\n",
            "Step 16100: Training loss 2.01777, Value loss 2.18477\n",
            "Step 16200: Training loss 2.00133, Value loss 2.16975\n",
            "Step 16300: Training loss 2.00887, Value loss 2.16230\n",
            "Step 16400: Training loss 2.02304, Value loss 2.17293\n",
            "Step 16500: Training loss 2.00749, Value loss 2.17164\n",
            "Step 16600: Training loss 1.99668, Value loss 2.16731\n",
            "Step 16700: Training loss 1.99465, Value loss 2.15931\n",
            "Step 16800: Training loss 1.99009, Value loss 2.14866\n",
            "Step 16900: Training loss 2.00012, Value loss 2.16375\n",
            "Step 17000: Training loss 1.97487, Value loss 2.16688\n",
            "Step 17100: Training loss 1.98738, Value loss 2.15801\n",
            "Step 17200: Training loss 1.99632, Value loss 2.16059\n",
            "Step 17300: Training loss 2.00162, Value loss 2.16681\n",
            "Step 17400: Training loss 1.98923, Value loss 2.15576\n",
            "Step 17500: Training loss 1.98596, Value loss 2.15033\n",
            "Step 17600: Training loss 1.98269, Value loss 2.14878\n",
            "Step 17700: Training loss 1.96782, Value loss 2.14223\n",
            "Step 17800: Training loss 1.97204, Value loss 2.13872\n",
            "Step 17900: Training loss 1.98266, Value loss 2.14225\n",
            "Step 18000: Training loss 1.98741, Value loss 2.13862\n",
            "Step 18100: Training loss 1.97240, Value loss 2.14846\n",
            "Step 18200: Training loss 1.98280, Value loss 2.13877\n",
            "Step 18300: Training loss 1.97668, Value loss 2.13494\n",
            "Step 18400: Training loss 1.96335, Value loss 2.13864\n",
            "Step 18500: Training loss 1.95361, Value loss 2.13158\n",
            "Step 18600: Training loss 1.97092, Value loss 2.13125\n",
            "Step 18700: Training loss 1.96471, Value loss 2.12013\n",
            "Step 18800: Training loss 1.98211, Value loss 2.13180\n",
            "Step 18900: Training loss 1.97655, Value loss 2.12747\n",
            "Step 19000: Training loss 1.95277, Value loss 2.12526\n",
            "Step 19100: Training loss 1.96540, Value loss 2.12531\n",
            "Step 19200: Training loss 1.98369, Value loss 2.13594\n",
            "Step 19300: Training loss 1.97040, Value loss 2.12507\n",
            "Step 19400: Training loss 1.95668, Value loss 2.12124\n",
            "Step 19500: Training loss 1.95236, Value loss 2.11860\n",
            "Step 19600: Training loss 1.94373, Value loss 2.12951\n",
            "Step 19700: Training loss 1.95110, Value loss 2.12191\n",
            "Step 19800: Training loss 1.93738, Value loss 2.10967\n",
            "Step 19900: Training loss 1.94722, Value loss 2.11117\n",
            "Step 20000: Training loss 1.94605, Value loss 2.10090\n",
            "Step 20100: Training loss 1.95329, Value loss 2.11026\n",
            "Step 20200: Training loss 1.94417, Value loss 2.10843\n",
            "Step 20300: Training loss 1.94137, Value loss 2.09603\n",
            "Step 20400: Training loss 1.93972, Value loss 2.10604\n",
            "Step 20500: Training loss 1.93780, Value loss 2.10812\n",
            "Step 20600: Training loss 1.92627, Value loss 2.11194\n",
            "Step 20700: Training loss 1.94711, Value loss 2.08507\n",
            "Step 20800: Training loss 1.94048, Value loss 2.11055\n",
            "Step 20900: Training loss 1.93176, Value loss 2.11157\n",
            "Step 21000: Training loss 1.94330, Value loss 2.09336\n",
            "Step 21100: Training loss 1.92431, Value loss 2.10610\n",
            "Step 21200: Training loss 1.93305, Value loss 2.09170\n",
            "Step 21300: Training loss 1.93729, Value loss 2.10714\n",
            "Step 21400: Training loss 1.93583, Value loss 2.08804\n",
            "Step 21500: Training loss 1.92392, Value loss 2.09422\n",
            "Step 21600: Training loss 1.92858, Value loss 2.08674\n",
            "Step 21700: Training loss 1.93538, Value loss 2.09604\n",
            "Step 21800: Training loss 1.92609, Value loss 2.10087\n",
            "Step 21900: Training loss 1.92289, Value loss 2.09583\n",
            "Step 22000: Training loss 1.94431, Value loss 2.08862\n",
            "Step 22100: Training loss 1.94210, Value loss 2.09349\n",
            "Step 22200: Training loss 1.92154, Value loss 2.09074\n",
            "Step 22300: Training loss 1.93490, Value loss 2.09239\n",
            "Step 22400: Training loss 1.92422, Value loss 2.07556\n",
            "Step 22500: Training loss 1.91625, Value loss 2.09347\n",
            "Step 22600: Training loss 1.91853, Value loss 2.08725\n",
            "Step 22700: Training loss 1.91808, Value loss 2.08073\n",
            "Step 22800: Training loss 1.90180, Value loss 2.07337\n",
            "Step 22900: Training loss 1.92958, Value loss 2.08653\n",
            "Step 23000: Training loss 1.90596, Value loss 2.06466\n",
            "Step 23100: Training loss 1.91230, Value loss 2.09252\n",
            "Step 23200: Training loss 1.90958, Value loss 2.08256\n",
            "Step 23300: Training loss 1.91479, Value loss 2.05865\n",
            "Step 23400: Training loss 1.90008, Value loss 2.07756\n",
            "Step 23500: Training loss 1.90628, Value loss 2.09062\n",
            "Step 23600: Training loss 1.90393, Value loss 2.07781\n",
            "Step 23700: Training loss 1.92143, Value loss 2.07460\n",
            "Step 23800: Training loss 1.91062, Value loss 2.07853\n",
            "Step 23900: Training loss 1.89871, Value loss 2.05223\n",
            "Step 24000: Training loss 1.90427, Value loss 2.06960\n",
            "Step 24100: Training loss 1.89298, Value loss 2.06454\n",
            "Step 24200: Training loss 1.89922, Value loss 2.06299\n",
            "Step 24300: Training loss 1.89702, Value loss 2.07026\n",
            "Step 24400: Training loss 1.90186, Value loss 2.05615\n",
            "Step 24500: Training loss 1.89134, Value loss 2.06762\n",
            "Step 24600: Training loss 1.90157, Value loss 2.08557\n",
            "Step 24700: Training loss 1.88413, Value loss 2.06730\n",
            "Step 24800: Training loss 1.89680, Value loss 2.05119\n",
            "Step 24900: Training loss 1.88930, Value loss 2.05555\n",
            "Step 25000: Training loss 1.88876, Value loss 2.05654\n",
            "Step 25100: Training loss 1.90816, Value loss 2.06191\n",
            "Step 25200: Training loss 1.90296, Value loss 2.04977\n",
            "Step 25300: Training loss 1.89323, Value loss 2.06029\n",
            "Step 25400: Training loss 1.88999, Value loss 2.07197\n",
            "Step 25500: Training loss 1.89243, Value loss 2.05637\n",
            "Step 25600: Training loss 1.89593, Value loss 2.05840\n",
            "Step 25700: Training loss 1.88660, Value loss 2.05514\n",
            "Step 25800: Training loss 1.88966, Value loss 2.04302\n",
            "Step 25900: Training loss 1.89550, Value loss 2.05079\n",
            "Step 26000: Training loss 1.88659, Value loss 2.05388\n",
            "Step 26100: Training loss 1.89304, Value loss 2.04603\n",
            "Step 26200: Training loss 1.90564, Value loss 2.05310\n",
            "Step 26300: Training loss 1.88382, Value loss 2.05251\n",
            "Step 26400: Training loss 1.88397, Value loss 2.03646\n",
            "Step 26500: Training loss 1.89050, Value loss 2.06156\n",
            "Step 26600: Training loss 1.87370, Value loss 2.04035\n",
            "Step 26700: Training loss 1.89094, Value loss 2.03804\n",
            "Step 26800: Training loss 1.87691, Value loss 2.03812\n",
            "Step 26900: Training loss 1.86198, Value loss 2.03574\n",
            "Step 27000: Training loss 1.85468, Value loss 2.02996\n",
            "Step 27100: Training loss 1.86856, Value loss 2.02532\n",
            "Step 27200: Training loss 1.87561, Value loss 2.03481\n",
            "Step 27300: Training loss 1.87497, Value loss 2.03691\n",
            "Step 27400: Training loss 1.86125, Value loss 2.04254\n",
            "Step 27500: Training loss 1.87210, Value loss 2.04086\n",
            "Step 27600: Training loss 1.87044, Value loss 2.03502\n",
            "Step 27700: Training loss 1.86761, Value loss 2.03109\n",
            "Step 27800: Training loss 1.86108, Value loss 2.04247\n",
            "Step 27900: Training loss 1.87316, Value loss 2.03041\n",
            "Step 28000: Training loss 1.88144, Value loss 2.03773\n",
            "Step 28100: Training loss 1.86592, Value loss 2.04208\n",
            "Step 28200: Training loss 1.87553, Value loss 2.02067\n",
            "Step 28300: Training loss 1.87154, Value loss 2.02959\n",
            "Step 28400: Training loss 1.86435, Value loss 2.04090\n",
            "Step 28500: Training loss 1.86871, Value loss 2.02836\n",
            "Step 28600: Training loss 1.86072, Value loss 2.02546\n",
            "Step 28700: Training loss 1.86964, Value loss 2.02359\n",
            "Step 28800: Training loss 1.83691, Value loss 2.01171\n",
            "Step 28900: Training loss 1.86074, Value loss 2.00864\n",
            "Step 29000: Training loss 1.85025, Value loss 2.01786\n",
            "Step 29100: Training loss 1.86515, Value loss 2.02597\n",
            "Step 29200: Training loss 1.86548, Value loss 2.02941\n",
            "Step 29300: Training loss 1.86806, Value loss 2.02716\n",
            "Step 29400: Training loss 1.84744, Value loss 2.01808\n",
            "Step 29500: Training loss 1.84215, Value loss 2.02778\n",
            "Step 29600: Training loss 1.86425, Value loss 2.02252\n",
            "Step 29700: Training loss 1.85546, Value loss 2.01364\n",
            "Step 29800: Training loss 1.85686, Value loss 2.02438\n",
            "Step 29900: Training loss 1.85963, Value loss 2.01301\n",
            "Step 29999: Training loss 1.84878, Value loss 2.00339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to check what TrumpGPT has cooked up (Most likely very bad lol and not very much training due to hardware constraints)"
      ],
      "metadata": {
        "id": "1GRTakCd58cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Generate a context for the model to began to decode and generate\n",
        "print(decode(m.generate(context, max_new_tokens=3000)[0].tolist())) # Here is the generation, increase the max tokens to get a longer string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9kxZqFX6Aog",
        "outputId": "f980816c-ae78-43a3-9d65-789bbc45d24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"Ie in Namps! Bold Jast Hille Obage jouluse Dona So is I moup oncorese teding he's allional us really morning.\"\n",
            "Gef with an misgets! Tike it for!\n",
            "\"Volion 2 Worpete --20urne.\"\" Thanks @mibmcishtch Rorig so like it leater.\"\n",
            "\"\"\"@2olinaJanaz:TrutifeNNFS\"\"\"\n",
            "\"\"\"@Holfuun_quaker @unbeSJohn11: “@RotFaod @realDonaldTrump @Beffrian_feents Probauti http://t.co/xWQvb7oCG\n",
            "\"\"\"@Ratiteo7Fin2Mons Twatcho/NBR38ATIvF46 NYFLL NAG: http://rYht6gY:/\n",
            "\"\"\"@railynjTy6: @realDonaldTrump. I comme – but willion.\"\"\"\n",
            "\"@gnya2016: Why @realDonaldTrump What You by Megin?\n",
            "“Trump not.They!!! John, I ame sawing Mourty peeprive mose agree polloves.\n",
            "@mannuma_mmin Christons Agenticons of Risery Noo with the I @sanids Mexand Time mis sechie Sappera great tis.\n",
            ".@Penninikesude: @realDonaldTrump in keep is a great agroe a dedinatence up the Critist by that you tomorivievati: @AIS74QLKEF Stang Repora gresibout and noot sup cheasilys enfort-torry! #Pess#Cell Jeb StaIrate Hiliss in Doral, aboubt cisive to gain!\"\" Thanks.\n",
            "It's liebers- appoble ObamaraAnnied it, said 201%\"\"\"\n",
            "\"\"\"@itHHitlh:?\n",
            "\"Just pleone soongin muck thing\"\n",
            "\"\"\"@jadySPieanKerPJBC NY I's A DFINKI #TreebatGrevBos Fox geasod co -up  gewnound, \"\"whem anying it.\n",
            "\"\"\"In, ObamaCintoTrump uredo at Monod.\n",
            "\"“Straving peun is beenied or welt\"\" our lave shows at Neming for vote Iombari, this got 878: http://t.co/5a91:2kAD8\n",
            "Prendon: @realDonaldTrump Trump @Bisrifut. I his me knory plious expetill mosn't white Can Sying truts #TwakeAmer Prenesiticaguare. #Mas#TonPakesFuratic andly sime.\n",
            "Edicks your don't Agree, Bob gettive\n",
            "\"\"\"@FonegcNe undi by diened the becaution Cormine in Nowd Trump I ganot reation He anrounds Trump. Whate your\"\"\"\n",
            "\"\"\"@iilbolgeining1, Trump @TrumpFolers for Rogelicans. Panatcion Heathing is farbe troomy Mayimnay's gooding anly in baseng in oftheri\"\"\"\n",
            "\"\"\"@uelnlchtmep: @realDonaldTrump\"\n",
            "\"\"\"@waverricVjoo, can't time dis Caling has was a cland 216.\n",
            "\"Wighte your Repaire. The the Homi haper airembon --go Somise sually @Brai4: @realDonaldTrump the satiizing ilmad \"\"ars Ats, dinLicemors inding too great's good to alloats on the was concrent.\"\" -- The clas very just.\n",
            "Pen @MyorryRiva. @TrumpIS $0D Pesive. #Grejee\n",
            "\"\"\"@lwintarSccShLEC hotel the potero, from Plake really above great at her or ingeriatient by @Bearrial9 Ripayors. Mexicantate. Estior: ......itt://t,FndYEigWNV\"\"\"\n",
            "\"\"\"@dirispazi\n",
            "Cointe the wited. Not we your time toule does and broing and is.\n",
            ".@DasSamQAnA Thates. https://t.co/frM_PbMztwkH!\n",
            "\"In chancran’t this a \"\"out Intles.\"\" Fliste last you tile is negide all raaloton deasons. Very at trations Trump 20://t.co/0sJWFPThm With #Fubseh wen famm?\"\n",
            "\"\"\"@NallotAgrn to pactechez wret wary levendieum!\"\n",
            "\"\"\"@miarelybrnGrnThe youre: @realDonaldTrump @Presidoool Fox, Trump Don't hims it. Veades Eass Immelion whek of the show.. They adrease Thells, qupick\"\n",
            "\"Entrue, you ado ame buwadd it lext very. Thank yours in canced Care to must me time.\n",
            "I love wated your foox than. yteaks!\"\n",
            "\"\"\"@EriveiuBkbrrootess. Why you Obama Uprices\n",
            "\"Via self the at i\n"
          ]
        }
      ]
    }
  ]
}